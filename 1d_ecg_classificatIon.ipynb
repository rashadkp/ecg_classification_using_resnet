{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jyIMzGSA-_oZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import wfdb\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "A43FYGHX_iX-"
   },
   "outputs": [],
   "source": [
    "#defenition of data path and excel file path\n",
    "path = '/home/abhishek/rashad_internship/Physionet/ptb-xl-1.0.3/'\n",
    "excel = '/home/abhishek/rashad_internship/Physionet/ptb-xl-1.0.3/ptbxl_database.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tozT3NrF_kAl"
   },
   "outputs": [],
   "source": [
    "#custom class definition\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import wfdb\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "class Custom_class(Dataset):\n",
    "    def __init__(self, excelfile, path, num_data, transform=None, data_split='train', fold=None):\n",
    "        self.dat = pd.read_csv(excelfile)\n",
    "        self.col = self.dat['filename_hr']  # only 500 hz files are used for training\n",
    "        self.label = self.dat['scp_codes']  # used for labeling\n",
    "        self.strat_fold = self.dat['strat_fold']  # Load strat_fold column\n",
    "        self.path = path\n",
    "        self.transform = transform  # Initialize the transform attribute\n",
    "        self.num_data = num_data\n",
    "        self.data_split = data_split\n",
    "        self.fold = fold\n",
    "\n",
    "        if self.data_split == 'train':\n",
    "            self.indices = [idx for idx in range(self.num_data) if (self.strat_fold[idx] != fold)]\n",
    "        elif self.data_split == 'test':\n",
    "            self.indices = [idx for idx in range(self.num_data) if (self.strat_fold[idx] == fold)]\n",
    "        elif self.data_split == 'val':\n",
    "            self.indices = [idx for idx in range(self.num_data) if (self.strat_fold[idx] == fold)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.indices[idx]  # Adjust index to match filtered data\n",
    "        y, _ = wfdb.rdsamp(self.path + self.col[idx])  # Use channel 0\n",
    "        y = y.astype(np.float32)\n",
    "        y = np.transpose(y)\n",
    "\n",
    "        # Apply filtering\n",
    "        y = self.bandpass_filter(y, 1, 47, 500)  # applying BPF\n",
    "\n",
    "        # Normalize using z-score\n",
    "        y = self.z_score_normalize(y)\n",
    "        y = y.astype(np.float32)\n",
    "\n",
    "        scp_code_dict = ast.literal_eval(self.label[idx])  # Fetching label from the scp_codes column\n",
    "\n",
    "        # Check if the first key is 'NORM' and assign the label accordingly\n",
    "        first_key = max(scp_code_dict, key=scp_code_dict.get)  # one key in scp_code dictionary with highest value is considered as label\n",
    "        label = 0 if first_key == 'NORM' else 1  # if label is NORM then encoded as 1 else 0\n",
    "\n",
    "        if self.transform:\n",
    "            y = self.transform(y)\n",
    "\n",
    "        return y[0, :, :], label\n",
    "\n",
    "    def bandpass_filter(self, data, lowcut, highcut, fs, order=3):\n",
    "        nyquist = 0.5 * fs\n",
    "        low = lowcut / nyquist\n",
    "        high = highcut / nyquist\n",
    "        b, a = butter(order, [low, high], btype='band')\n",
    "        y = filtfilt(b, a, data, axis=1)\n",
    "        return y\n",
    "\n",
    "    def z_score_normalize(self, data):\n",
    "        mean = np.mean(data, axis=1, keepdims=True)\n",
    "        std = np.std(data, axis=1, keepdims=True)\n",
    "        normalized_data = (data - mean) / std\n",
    "        return normalized_data\n",
    "\n",
    "# Example usage\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# For training data, TOTAL 7000 data is used for TRAINING AND TESTING\n",
    "train_dataset = Custom_class(excel, path, num_data=7000, transform=transform, data_split='train',fold=10)\n",
    "\n",
    "# For test data\n",
    "test_dataset = Custom_class(excel, path, num_data=7000, transform=transform, data_split='test',fold=10)\n",
    "\n",
    "# For validation data\n",
    "val_dataset = Custom_class(excel, path, num_data=7000, transform=transform, data_split='val',fold=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6171\n",
      "829\n",
      "829\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XI7KUi3h_-iL"
   },
   "outputs": [],
   "source": [
    "#MODEL DEFINITION\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the Res_Block_1\n",
    "class ResBlock1(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResBlock1, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=2, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv3 = nn.Conv1d(out_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm1d(out_channels)\n",
    "        self.adjust_channels = nn.Conv1d(in_channels, out_channels, kernel_size=2, stride=2, padding=1)\n",
    "        self.adjust_bn = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.adjust_channels(x)\n",
    "        shortcut = self.adjust_bn(shortcut)\n",
    "\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)))\n",
    "        x = self.bn3(self.conv3(x))\n",
    "        x = x + shortcut\n",
    "        x = F.leaky_relu(x)\n",
    "        return x\n",
    "\n",
    "# Define the Res_Block_2\n",
    "class ResBlock2(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResBlock2, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv3 = nn.Conv1d(out_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm1d(out_channels)\n",
    "        self.adjust_channels = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.adjust_bn = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.adjust_channels(x)\n",
    "        shortcut = self.adjust_bn(shortcut)\n",
    "\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)))\n",
    "        x = self.bn3(self.conv3(x))\n",
    "        x = x + shortcut\n",
    "        x = F.leaky_relu(x)\n",
    "        return x\n",
    "\n",
    "# Define the complete ResNet-50 model with Self-Attention\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, input_channels=12, num_classes=2):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(ResBlock1, 64, 128, 1)\n",
    "        self.layer2 = self._make_layer(ResBlock2, 128, 128, 2)\n",
    "        self.layer3 = self._make_layer(ResBlock1, 128, 256, 1)\n",
    "        self.layer4 = self._make_layer(ResBlock2, 256, 256, 3)\n",
    "        self.layer5 = self._make_layer(ResBlock1, 256, 512, 1)\n",
    "        self.layer6 = self._make_layer(ResBlock2, 512, 512, 5)\n",
    "        self.layer7 = self._make_layer(ResBlock1, 512, 1024, 1)\n",
    "        self.layer8 = self._make_layer(ResBlock2, 1024, 1024, 2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(p=0.5) #added to improve generalization\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "    def _make_layer(self, block, in_channels, out_channels, blocks):\n",
    "        layers = []\n",
    "        layers.append(block(in_channels, out_channels))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "        x = self.layer8(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = torch.squeeze(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model and print the summary\n",
    "model = ResNet50(input_channels=12, num_classes=1)\n",
    "\n",
    "# Check if CUDA is available and move the model to the GPU if it is\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_P3pWpWNGj7x"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True,num_workers=2)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True,num_workers=2)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "812\n"
     ]
    }
   ],
   "source": [
    "test_dataset = Custom_class(excel, path, num_data=7000, transform=transform, data_split='test',fold=9)\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True,num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math\n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < 10:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return math.exp(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rclVOfWEF8lt",
    "outputId": "cc132c7e-9ecb-4353-c11f-88ce1595e523"
   },
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "# from torch.optim import lr_scheduler\n",
    "# import copy\n",
    "\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) \n",
    "# # scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "# scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# num_epochs = 20\n",
    "# best_model_wts = copy.deepcopy(model.state_dict())\n",
    "# best_acc = 0.0\n",
    "\n",
    "\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.optim import lr_scheduler\n",
    "# import copy\n",
    "\n",
    "# # Assuming you have defined your model, criterion, optimizer, scheduler, num_epochs, best_model_wts, and best_acc\n",
    "\n",
    "# # Define your validation function\n",
    "# def validate_model(model, dataloader, criterion):\n",
    "#     model.eval()  # Set model to evaluation mode\n",
    "#     running_loss = 0.0\n",
    "#     running_corrects = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in dataloader:\n",
    "#             inputs = inputs.to(device)\n",
    "#             labels = labels.to(device)\n",
    "#             labels = labels.float()\n",
    "\n",
    "#             outputs = model(inputs)\n",
    "#             preds = torch.round(outputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "\n",
    "#             running_loss += loss.item() * inputs.size(0)\n",
    "#             running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#     epoch_loss = running_loss / len(dataloader.dataset)\n",
    "#     epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "    \n",
    "#     return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()  # Set model to training mode\n",
    "#     running_loss = 0.0\n",
    "#     running_corrects = 0\n",
    "\n",
    "#     for inputs, labels in train_dataloader:\n",
    "#         inputs = inputs.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         labels = labels.float()\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         outputs = model(inputs)\n",
    "#         preds = torch.round(outputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item() * inputs.size(0)\n",
    "#         running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#     epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "#     epoch_acc = running_corrects.double() / len(train_dataloader.dataset)\n",
    "#     scheduler.step()\n",
    "\n",
    "#     print(f'Training - Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "#     # Validate the model\n",
    "#     val_loss, val_acc = validate_model(model, validation_dataloader, criterion)\n",
    "#     print(f'Validation - Epoch {epoch+1}/{num_epochs}, Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}')\n",
    "\n",
    "#     # Deep copy the model if the current validation accuracy is the best so far\n",
    "#     if val_acc > best_acc:\n",
    "#         best_acc = val_acc\n",
    "#         best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#         # Save the best model\n",
    "#         torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "# # Load best model weights\n",
    "# model.load_state_dict(best_model_wts)\n",
    "\n",
    "# print(f\"Training complete. Best validation accuracy: {best_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1/20, Loss: 0.4907, Accuracy: 0.7675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Epoch 1/20, Loss: 0.4827, Accuracy: 0.7582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/20:  55%|██████████████████████████████████████████████████████████████████████████████████▎                                                                    | 108/198 [00:23<00:18,  4.86it/s]"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm  # Import tqdm for progress bars\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "num_epochs = 20\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "k = 6\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def validate_model(model, dataloader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in tqdm(dataloader, desc=\"Validation\", leave=False):  # Add tqdm for validation\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = labels.float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.round(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# List to store validation accuracies for each fold\n",
    "val_accuracies = []\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "for fold in range(1, k+1):\n",
    "    print(f'Fold {fold}/{k}')\n",
    "    \n",
    "    # Initialize the datasets and dataloaders for the current fold\n",
    "    train_dataset = Custom_class(excel, path, num_data=7000, transform=transform, data_split='train', fold=fold)\n",
    "    validation_dataset = Custom_class(excel, path, num_data=7000, transform=transform, data_split='val', fold=fold)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    validation_dataloader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Reset model, optimizer, and scheduler for each fold\n",
    "    model = ResNet50(input_channels=12, num_classes=1)\n",
    "\n",
    "    # Check if CUDA is available and move the model to the GPU if it is\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\", leave=False):  # Add tqdm for training\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            labels = labels.float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.round(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_dataloader.dataset)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f'Training - Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "        # Validate the model\n",
    "        val_loss, val_acc = validate_model(model, validation_dataloader, criterion)\n",
    "        print(f'Validation - Epoch {epoch+1}/{num_epochs}, Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}')\n",
    "\n",
    "        # Deep copy the model if the current validation accuracy is the best so far\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), f\"best_model_fold_{fold}.pth\")\n",
    "    \n",
    "    # Store the validation accuracy for the current fold\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "# Calculate the average validation accuracy across all folds\n",
    "avg_val_acc = sum(val_accuracies) / k\n",
    "\n",
    "print(f\"Training complete. Best validation accuracy: {best_acc:.4f}\")\n",
    "print(f\"Average validation accuracy over {k} folds: {avg_val_acc:.4f}\")\n",
    "\n",
    "# Load best model weights\n",
    "if best_model_wts:\n",
    "    model.load_state_dict(best_model_wts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P08h1mCVhVPI",
    "outputId": "977f2580-2673-4d9d-b400-24e941f2b885"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "model = ResNet50(input_channels=12, num_classes=1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('best_model_fold_1.pth'))\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            preds = torch.round(outputs)\n",
    "            preds = preds.int()\n",
    "            # _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    return accuracy, cm\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `model` is your PyTorch model and `test_loader` is your test data loader\n",
    "accuracy, confusion_matrix = test_model(model, test_dataloader)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 549
    },
    "id": "IpeF9mgyldMf",
    "outputId": "da2cda34-7052-4eb0-8324-61dfe3b7fba3"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
