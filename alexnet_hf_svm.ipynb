{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import wfdb\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import ast\n",
    "import scipy.stats as stats\n",
    "import statsmodels\n",
    "import HiguchiFractalDimension as hfd\n",
    "import pywt\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import neurokit2 as nk\n",
    "import wandb\n",
    "from ssqueezepy import ssq_cwt\n",
    "import cv2\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to compute AR Coefficient,Wavelet variance,shannon entropy,Fractal Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(signal):\n",
    "    # Auto-regressive coefficients\n",
    "    ar_order = 20\n",
    "    ar_model = statsmodels.tsa.ar_model.AutoReg(signal,lags=20)\n",
    "    ar_fit = ar_model.fit()\n",
    "    ar_coeffs = ar_fit.params\n",
    "\n",
    "    # Shannon's entropy\n",
    "    prob_density, _ = np.histogram(signal, bins=256, density=True)\n",
    "    prob_density = prob_density[prob_density > 0]  # Remove zero probabilities\n",
    "    entropy = -np.sum(prob_density * np.log2(prob_density))\n",
    "\n",
    "    # Wavelet variance (using 'db2')\n",
    "    coeffs = pywt.wavedec(signal, 'db2')\n",
    "    wavelet_var = np.var(coeffs[-1])  # Variance of the last detail coefficients\n",
    "\n",
    "    # Higuchi fractal dimension\n",
    "    fractal_dim = hfd.hfd(signal)\n",
    "\n",
    "    # Mean\n",
    "    mean_val = np.mean(signal)\n",
    "\n",
    "    # Kurtosis\n",
    "    kurtosis_val = stats.kurtosis(signal)\n",
    "\n",
    "    # Skewness\n",
    "    skewness_val = stats.skew(signal)\n",
    "    # Combine all features into a single vector\n",
    "    features = np.concatenate([\n",
    "        ar_coeffs,\n",
    "        [entropy],\n",
    "        [wavelet_var],\n",
    "        [fractal_dim],\n",
    "        [mean_val],\n",
    "        [kurtosis_val],\n",
    "        [skewness_val]\n",
    "    ])\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition of data path and excel file path\n",
    "path = '/home/abhishek/rashad_internship/Physionet/ptb-xl-1.0.3/'\n",
    "excel = '/home/abhishek/rashad_internship/Physionet/ptb-xl-1.0.3/ptbxl_database.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First 1000 files are read.\n",
    "df = pd.read_csv(excel)\n",
    "df = df[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alexnet Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load the pre-trained AlexNet model\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "\n",
    "# Modify the model to extract features\n",
    "# We will keep all layers except the final classifier layers\n",
    "class AlexNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNetFeatureExtractor, self).__init__()\n",
    "        self.features = alexnet.features\n",
    "        self.avgpool = alexnet.avgpool\n",
    "        # self.classifier = nn.Sequential(*list(alexnet.classifier.children())[:-1]) # Remove the last classifier layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        # x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the modified AlexNet model\n",
    "model = AlexNetFeatureExtractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to convert 1D signal to spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def onedim_to_twodim(data):\n",
    "    # Perform Continuous Wavelet Transform (CWT) using Morlet wavelet\n",
    "    # Replace with your actual CWT function (ssq_cwt) or use another STFT method\n",
    "    Txo, _, Wxo, scales_xo, _ = ssq_cwt(data, 'morlet')\n",
    "    Wxo /= np.sqrt(scales_xo)\n",
    "    \n",
    "    # Plot the CWT coefficients and save as an image\n",
    "    plt.imshow(np.abs(Wxo), aspect='auto', cmap='jet')\n",
    "    plt.axis('off')  # Remove axes for a cleaner image\n",
    "    plt.savefig('temp_spectrogram.png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "    # Load the saved image\n",
    "    image = cv2.imread('temp_spectrogram.png')\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "\n",
    "    # Resize the image to 224x224\n",
    "    resized_image = cv2.resize(image, (224, 224))\n",
    "\n",
    "    # Convert to tensor and add batch dimension\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard normalization for ImageNet\n",
    "    ])\n",
    "    tensor_image = transform(resized_image).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    return tensor_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction of signal using Alexnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhishek/miniconda3/envs/rashadenv/lib/python3.9/site-packages/ssqueezepy/ssqueezing.py:253: RuntimeWarning: invalid value encountered in divide\n",
      "  w = np.imag(dWx / Wx) / (2*pi)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "x_data = []\n",
    "y_data = []\n",
    "count = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "        # Check if we have reached the desired number of data points\n",
    "        \n",
    "        \n",
    "        # Read the signal data\n",
    "    filename = row['filename_hr']\n",
    "    signal, _ = wfdb.rdsamp(path + filename, channels=[0])\n",
    "    signal = signal.flatten().astype(np.float32)\n",
    "    # Calculate features\n",
    "    im = onedim_to_twodim(signal)\n",
    "    # features = compute_features(signal)\n",
    "    im = im.to(device)\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        features = model(im)\n",
    "    features = features.cpu()\n",
    "    features = features.numpy()\n",
    "\n",
    "    x_data.append(features)\n",
    "\n",
    "        # Determine the label\n",
    "    scp_code_dict = ast.literal_eval(row['scp_codes'])\n",
    "    first_key = max(scp_code_dict, key=scp_code_dict.get)\n",
    "    label = 0 if first_key == 'NORM' else 1\n",
    "    y_data.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data_np = np.array(x_data)\n",
    "y_data_np = np.array(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the signal from 1000,1,9216 to 1000,9216\n",
    "x_data_np = np.reshape(x_data_np, (1000, 9216))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting Top 1000 features using PCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df = pd.DataFrame(x_data_np)\n",
    "y_train_series = pd.Series(y_data_np)\n",
    "\n",
    "# Compute Pearson correlation for each feature\n",
    "correlations = x_train_df.apply(lambda col: col.corr(y_train_series))\n",
    "\n",
    "# Select the top k features based on absolute correlation\n",
    "k = 1000  # Number of top features to select\n",
    "top_k_features = correlations.abs().sort_values(ascending=False).head(k).index\n",
    "\n",
    "# Extract the top k features from the original data\n",
    "x_train_selected = x_train_df.iloc[:, top_k_features].values\n",
    "\n",
    "# Convert the selected features back to tensor if needed\n",
    "# x_train_selected_tensor = torch.tensor(x_train_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction using Handcrafted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset2 = []\n",
    "y_data_2 = []\n",
    "\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "        # Check if we have reached the desired number of data points\n",
    "        \n",
    "        \n",
    "        # Read the signal data\n",
    "    filename = row['filename_hr']\n",
    "    signal, _ = wfdb.rdsamp(path + filename, channels=[0])\n",
    "    signal = signal.flatten().astype(np.float32)\n",
    "    # Calculate features\n",
    "    # im = onedim_to_twodim(signal)\n",
    "    features = compute_features(signal)\n",
    "    featureset2.append(features)\n",
    "\n",
    "        # Determine the label\n",
    "    scp_code_dict = ast.literal_eval(row['scp_codes'])\n",
    "    first_key = max(scp_code_dict, key=scp_code_dict.get)\n",
    "    label = 0 if first_key == 'NORM' else 1\n",
    "    y_data_2.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset2_np = np.array(featureset2)\n",
    "y_data_2_np = np.array(y_data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining HF and CNN Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_array = np.concatenate((x_train_selected, featureset2_np), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7650\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert to pandas DataFrame for easier manipulation\n",
    "x_train_selected_df = pd.DataFrame(combined_array)\n",
    "# y_train_series = pd.Series(y_data_np)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x_train_selected_df, y_train_series, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train an SVM classifier\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rashadenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
